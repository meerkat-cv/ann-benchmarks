# -*- coding: utf-8 ; org-export-babel-evaluate: t; org-confirm-babel-evaluate: nil; org-image-actual-width: 600;-*-
# -*- mode: org -*-
#+AUTHOR: Julio Toss
#+EMAIL: julio@meerkat.com.br
#+STARTUP: indent 
#+STARTUP: logdrawer hideblocks
#+OPTIONS: html-postamble:nil f:nil broken-links:mark H:5 toc:nil todo:nil ^:{}
#+PROPERTY: header-args :cache no :eval never-export
#+SEQ_TODO: TODO INPROGRESS(i) WAITING(@) | DONE NOTE DEFERRED(@) CANCELED(@)


* DONE Dataset - vgg-512-euclidean

Dataset from face images : https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/
Face descriptors (feature vectors) were extracted with frapi's method ( arcFace )

#+begin_src python :results output table :exports results :eval never
import numpy as np
import h5py

filename = "../data/vgg-512-euclidean.hdf5"
dataset = h5py.File(filename, "r")

train = set(dataset['train_lbl'])
test = set(dataset['test_lbl'])
diff = (test - train) 

print("Vectors type:", dataset['train'].dtype)
print("Train Vectors:", dataset['train'].shape)
print("Test Vectors:", dataset['test'].shape)

print("Train Labels:", len(train))
print("Test Labels:", len(test))

print("Untrained Labels: ", len(diff))

#+end_src

#+RESULTS:
: Vectors type: float64
: Train Vectors: (2701775, 512)
: Test Vectors: (10000, 512)
: Train Labels: 8631
: Test Labels: 5748
: Untrained Labels:  0

* Evaluation methodology 

For each algorithm configuration ( algorithm + parameters ):
- we build the indexing datastructure one time.
- we perform two test /runs/. In each /run/ the full train dataset is queried over the computed index.
- each query is executed individually ( no batch queries ).

Resource limits:
Each test configuration is executed in a docker container. 
Containers resources where limited to 1 CPU, unlimited memory and run timeout of 5 hours.


** Performance Metrics
*** Index
- Index build time: we measure the seconds to run the method's =fit= function.
- Index memory usage: we compute the difference between the Resident Set Size , before and after the fit function.

Sample snippet: 
#+begin_src sh :results output :exports both
t0 = time.time()
memory_usage_before = algo.get_memory_usage()
algo.fit(X_train)
build_time = time.time() - t0
index_size = algo.get_memory_usage() - memory_usage_before
#+end_src

*** Query Time ( Latency )

In each test run, we measure seconds (wall time) for each query individually. 
All queries' times in a single run are added up to the compute average query time.

We perform two test /runs/ and take the best average time, which is reported in our results.

Each individual query time are save to =.hdf5= files in case we need them for extra analysis.

** Quality Metrics

The output of the knn-query is a list of neighbors ids in the /Train/ dataset.

/Note:/ all the queries were executed on a *10-Nearest-Neighbors* index, so the response will always be a list of 10 candidates ordered by distance ( from smallest to largest ) 

From the list of candidates ids we generate a list candidate labels and compute the first position where the query label appears in the candidate list ( aka rank).

Rank computation:
#+begin_src python :results output :exports both
if label in query_labels:
    rank_q = neighbor_labels.index( label )
else:
    rank_q = float(inf)
#+end_src

We computed two rank based accuracy :

- Accuracy at Rank@1 :: =( queries with rank < 1 ) / num_queries=

- Accuracy at Rank@10 :: =( queries with rank < 10) / num_queries=


* Algorithms                                                       :noexport:
** Selected algorithm 
Faiss
HNSW

* Results

Raw results are available here : [[../results/vgg-512-euclidean.csv]]

#+begin_src R :results output :exports none :session 
options(crayon.enabled = FALSE)
options(dplyr.width=Inf)

library(tidyverse)
df = read_delim("../results/vgg-512-euclidean.csv", delim=',', trim_ws = TRUE )
head(df)
#+end_src


** Query Time vs accuracy-Rank@1

#+begin_src sh :results output none :exports none :session foo
cd ~/Projects/ann-benchmarks
python3 plot.py --dataset vgg-512-euclidean -y queryTime -x accuracy-R@1 -o reports/img/vgg-512-euclidean-query-R1.svg -Y
#+end_src

# [[file:img/vgg-512-euclidean-query-R1.svg]]

#+begin_src R :results output graphics :file ./img/vgg-512-euclidean-query-R1.svg :exports results :width 8 :height 5 :session 
library(rPref)

df %>% 
    filter(`accuracy-R@1` > 0.9) %>%
    group_by(algorithm) -> dff

dff %>%
    psel(high(`accuracy-R@1`) * low(queryTime)) -> df_sky

dff %>%  
    ggplot( aes(x=`accuracy-R@1`, y=queryTime, color=algorithm)) +
    geom_point(alpha=0.3) +
    geom_line(data = df_sky) +
    xlim(NA,0.98) +
    scale_y_continuous(trans='log10')
#+end_src

#+RESULTS:
[[file:./img/vgg-512-euclidean-query-R1.svg]]

** Query Time vs accuracy-Rank@10

#+begin_src sh :results output none :exports results :session foo
cd ~/Projects/ann-benchmarks
python3 plot.py --dataset vgg-512-euclidean -y queryTime -x accuracy-R@10 -o reports/img/vgg-512-euclidean-query-R10.svg -Y
#+end_src

[[file:img/vgg-512-euclidean-query-R10.svg]]

** Index Build Time vs accuracy-Rank@1

#+begin_src sh :results output none :exports results :session foo
cd ~/Projects/ann-benchmarks
python3 plot.py --dataset vgg-512-euclidean -y build -x accuracy-R@1 -o reports/img/vgg-512-euclidean-build-R1.svg -Y
#+end_src

[[file:img/vgg-512-euclidean-build-R1.svg]]

** Index Size vs accuracy-Rank@1

#+begin_src sh :results output none :exports results :session foo
cd ~/Projects/ann-benchmarks
python3 plot.py --dataset vgg-512-euclidean -y indexsize -x accuracy-R@1 -o reports/img/vgg-512-euclidean-IndexSize-R1.svg -Y
#+end_src

[[file:img/vgg-512-euclidean-IndexSize-R1.svg]]
